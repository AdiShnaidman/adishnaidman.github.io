<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Adi Shnaidman</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      max-width: 900px;
      margin: 80px auto;
      padding: 0 20px;
      line-height: 1.6;
      color: #111;
    }
    h1, h2 {
      line-height: 1.25;
    }
    h2 {
      margin-top: 2.5em;
    }
    ul {
      padding-left: 1.2em;
    }
    a {
      color: #0a66c2;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>

<body>

<h1>Adi Shnaidman</h1>
<p>
  <strong>Machine Learning Researcher</strong><br/>
  LLM Safety, Robustness, and Adversarial Machine Learning
</p>

<h2>About</h2>
<p>
  I am a Machine Learning Researcher focused on advancing the robustness,
  safety, and reliability of large language models.
  My work centers on understanding and mitigating adversarial behavior,
  misuse, and unintended failure modes in real-world deployments.
</p>

<h2>Current Work</h2>
<p>
  At <strong>DeepKeep</strong>, I research and build real-time detection and
  mitigation strategies to protect LLMs against adversarial prompts and
  behavioral vulnerabilities. This includes large-scale stress testing,
  evaluation framework design, and research-driven security solutions for
  production systems.
</p>

<h2>Research Interests</h2>
<ul>
  <li>LLM robustness, safety, and trustworthiness</li>
  <li>Adversarial prompting and prompt injection</li>
  <li>Evaluation and benchmarking of LLM behavior</li>
  <li>Model over-generation, misuse, and failure analysis</li>
  <li>Inference-time control and steering methods</li>
</ul>

<h2>Selected Publications</h2>
<ul>
  <li>
    <strong>Activation Steering for Masked Diffusion Language Models</strong><br/>
    arXiv, 2026
  </li>
  <li>
    <strong>BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts</strong><br/>
    NVIDIA GTC 2026 · Transactions on Machine Learning Research (TMLR)
  </li>
</ul>

<h2>Projects</h2>
<ul>
  <li>
    LLM adversarial detection and mitigation systems (DeepKeep)
  </li>
  <li>
    Large-scale evaluation pipelines for safety, hallucination, and misuse
  </li>
</ul>

<h2>Contact</h2>
<p>
  <a href="https://github.com/AdiShnaidman">GitHub</a> ·
  <a href="https://www.linkedin.com/in/adi-shnaidman/">LinkedIn</a>
</p>

</body>
</html>
